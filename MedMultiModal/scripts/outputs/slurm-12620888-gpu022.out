Wed May 15 18:42:33 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:06:00.0 Off |                    0 |
| N/A   38C    P8    16W /  70W |      0MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:86:00.0 Off |                    0 |
| N/A   36C    P8    15W /  70W |      0MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
SLURM_ARRAY_JOB_ID=
SLURM_JOBID=12620888
Running retrieval on ROCO...
/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transform.py:378: UserWarning: Unused augmentation cfg items, specify `use_timm` to use (['quilt_crop', 'num_views']).
  warnings.warn(f'Unused augmentation cfg items, specify `use_timm` to use ({list(aug_cfg_dict.keys())}).')
Models: [('ViT-B-32', '/projects/multimodal/checkpoints/hp_tuning/HPtuning_batchsize_32/checkpoints/epoch_6.pt')]
Datasets: ['roco']
Languages: ['en']
Running 'zeroshot_retrieval' on 'roco' with the model '/projects/multimodal/checkpoints/hp_tuning/HPtuning_batchsize_32/checkpoints/epoch_6.pt' on language 'en'
Number of parameters:  151277313
!!!Using cached dataset
Dataset size: 8176
Dataset split: test
/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transform.py:378: UserWarning: Unused augmentation cfg items, specify `use_timm` to use (['quilt_crop', 'num_views']).
  warnings.warn(f'Unused augmentation cfg items, specify `use_timm` to use ({list(aug_cfg_dict.keys())}).')
Models: [('ViT-B-32', '/projects/multimodal/checkpoints/hp_tuning/HPtuning_batchsize_32/checkpoints/epoch_6.pt')]
Datasets: ['roco']
Languages: ['en']
Running 'zeroshot_retrieval' on 'roco' with the model '/projects/multimodal/checkpoints/hp_tuning/HPtuning_batchsize_32/checkpoints/epoch_6.pt' on language 'en'
Number of parameters:  151277313
!!!Using cached dataset
Dataset size: 8176
Dataset split: test
0it [00:00, ?it/s]1it [00:07,  7.69s/it]2it [00:11,  5.57s/it]3it [00:14,  4.42s/it]3it [00:16,  5.56s/it]
Traceback (most recent call last):
  File "clip_benchmark/cli.py", line 693, in <module>
    sys.exit(main())  # pragma: no cover
  File "clip_benchmark/cli.py", line 280, in main
    main_eval(base)
  File "clip_benchmark/cli.py", line 398, in main_eval
    run(args)
  File "clip_benchmark/cli.py", line 558, in run
    amp=args.amp,
  File "/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/clip_benchmark/metrics/zeroshot_retrieval.py", line 54, in evaluate
    batch_texts_emb = F.normalize(model.encode_text(batch_texts_tok), dim=-1)
  File "/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/model.py", line 315, in encode_text
    x = self.transformer(x, attn_mask=self.attn_mask)
  File "/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transformer.py", line 390, in forward
    x = r(x, attn_mask=attn_mask)
  File "/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transformer.py", line 265, in forward
    x = x + self.ls_2(self.mlp(self.ln_2(x)))
  File "/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 684, in forward
    return F.gelu(input, approximate=self.approximate)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB (GPU 0; 14.75 GiB total capacity; 3.39 GiB already allocated; 764.81 MiB free; 5.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: gpu022: task 0: Exited with exit code 1
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** STEP 12620888.0 ON gpu022 CANCELLED AT 2024-05-15T18:43:57 ***
slurmstepd: error: *** JOB 12620888 ON gpu022 CANCELLED AT 2024-05-15T18:43:57 ***
