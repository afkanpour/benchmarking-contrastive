Tue May 14 13:56:21 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:17:00.0 Off |                    0 |
| N/A   38C    P0    64W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |
| N/A   38C    P0    63W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:B1:00.0 Off |                    0 |
| N/A   37C    P0    64W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:CA:00.0 Off |                    0 |
| N/A   38C    P0    61W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
SLURM_ARRAY_JOB_ID=
SLURM_JOBID=12598536
2024-05-14,13:56:35 | INFO | No latest resume checkpoint found in /checkpoint/yaspar/12598536/HPtuning_batchsize_32/checkpoints.
NCCL version 2.14.3+cuda11.7
2024-05-14,13:56:41 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2024-05-14,13:56:41 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2024-05-14,13:56:41 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2024-05-14,13:56:41 | INFO | Loading pretrained ViT-B-32 from OpenAI.
2024-05-14,13:56:41 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2024-05-14,13:56:41 | INFO | Loading pretrained ViT-B-32 from OpenAI.
2024-05-14,13:56:41 | INFO | Loading pretrained ViT-B-32 from OpenAI.
2024-05-14,13:56:41 | INFO | Loading pretrained ViT-B-32 from OpenAI.
/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transform.py:378: UserWarning: Unused augmentation cfg items, specify `use_timm` to use (['scale', 'quilt_crop', 'num_views']).
  warnings.warn(f'Unused augmentation cfg items, specify `use_timm` to use ({list(aug_cfg_dict.keys())}).')
2024-05-14,13:56:48 | INFO | Model:
2024-05-14,13:56:48 | INFO | CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (9): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (10): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (11): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
2024-05-14,13:56:48 | INFO | Params:
2024-05-14,13:56:48 | INFO |   accum_freq: 4
2024-05-14,13:56:48 | INFO |   aug_cfg: {'quilt_crop': True}
2024-05-14,13:56:48 | INFO |   batch_size: 32
2024-05-14,13:56:48 | INFO |   beta1: 0.9
2024-05-14,13:56:48 | INFO |   beta2: 0.98
2024-05-14,13:56:48 | INFO |   checkpoint_path: /checkpoint/yaspar/12598536/HPtuning_batchsize_32/checkpoints
2024-05-14,13:56:48 | INFO |   coca_caption_loss_weight: 2.0
2024-05-14,13:56:48 | INFO |   coca_contrastive_loss_weight: 1.0
2024-05-14,13:56:48 | INFO |   copy_codebase: False
2024-05-14,13:56:48 | INFO |   csv_caption_key: title
2024-05-14,13:56:48 | INFO |   csv_img_key: filepath
2024-05-14,13:56:48 | INFO |   csv_img_root: 
2024-05-14,13:56:48 | INFO |   csv_separator: ,
2024-05-14,13:56:48 | INFO |   dataset_resampled: False
2024-05-14,13:56:48 | INFO |   dataset_type: mixed
2024-05-14,13:56:48 | INFO |   ddp_static_graph: False
2024-05-14,13:56:48 | INFO |   debug: False
2024-05-14,13:56:48 | INFO |   delete_previous_checkpoint: False
2024-05-14,13:56:48 | INFO |   device: cuda:0
2024-05-14,13:56:48 | INFO |   dist_backend: nccl
2024-05-14,13:56:48 | INFO |   dist_url: env://
2024-05-14,13:56:48 | INFO |   distill: False
2024-05-14,13:56:48 | INFO |   distill_model: None
2024-05-14,13:56:48 | INFO |   distill_pretrained: None
2024-05-14,13:56:48 | INFO |   distributed: True
2024-05-14,13:56:48 | INFO |   epochs: 20
2024-05-14,13:56:48 | INFO |   epochs_cooldown: None
2024-05-14,13:56:48 | INFO |   eps: 1e-06
2024-05-14,13:56:48 | INFO |   force_custom_text: False
2024-05-14,13:56:48 | INFO |   force_image_size: None
2024-05-14,13:56:48 | INFO |   force_patch_dropout: None
2024-05-14,13:56:48 | INFO |   force_quick_gelu: False
2024-05-14,13:56:48 | INFO |   gather_with_grad: True
2024-05-14,13:56:48 | INFO |   grad_checkpointing: False
2024-05-14,13:56:48 | INFO |   grad_clip_norm: None
2024-05-14,13:56:48 | INFO |   horovod: False
2024-05-14,13:56:48 | INFO |   image_contrast: False
2024-05-14,13:56:48 | INFO |   image_interpolation: None
2024-05-14,13:56:48 | INFO |   image_mean: None
2024-05-14,13:56:48 | INFO |   image_resize_mode: None
2024-05-14,13:56:48 | INFO |   image_std: None
2024-05-14,13:56:48 | INFO |   imagenet_v2: None
2024-05-14,13:56:48 | INFO |   imagenet_val: None
2024-05-14,13:56:48 | INFO |   local_loss: False
2024-05-14,13:56:48 | INFO |   local_rank: 0
2024-05-14,13:56:48 | INFO |   lock_image: False
2024-05-14,13:56:48 | INFO |   lock_image_freeze_bn_stats: False
2024-05-14,13:56:48 | INFO |   lock_image_unlocked_groups: 0
2024-05-14,13:56:48 | INFO |   lock_text: False
2024-05-14,13:56:48 | INFO |   lock_text_decoder: False
2024-05-14,13:56:48 | INFO |   lock_text_freeze_layer_norm: False
2024-05-14,13:56:48 | INFO |   lock_text_unlocked_layers: 0
2024-05-14,13:56:48 | INFO |   log_every_n_steps: 100
2024-05-14,13:56:48 | INFO |   log_level: 20
2024-05-14,13:56:48 | INFO |   log_local: False
2024-05-14,13:56:48 | INFO |   log_path: /checkpoint/yaspar/12598536/HPtuning_batchsize_32/out.log
2024-05-14,13:56:48 | INFO |   logs: /checkpoint/yaspar/12598536/
2024-05-14,13:56:48 | INFO |   lr: 5e-05
2024-05-14,13:56:48 | INFO |   lr_cooldown_end: 0.0
2024-05-14,13:56:48 | INFO |   lr_cooldown_power: 1.0
2024-05-14,13:56:48 | INFO |   lr_scheduler: cosine
2024-05-14,13:56:48 | INFO |   mask_contrast: False
2024-05-14,13:56:48 | INFO |   model: ViT-B-32
2024-05-14,13:56:48 | INFO |   name: HPtuning_batchsize_32
2024-05-14,13:56:48 | INFO |   no_set_device_rank: False
2024-05-14,13:56:48 | INFO |   pathmnist: False
2024-05-14,13:56:48 | INFO |   precision: amp
2024-05-14,13:56:48 | INFO |   pretrained: openai
2024-05-14,13:56:48 | INFO |   pretrained_image: False
2024-05-14,13:56:48 | INFO |   quilt_roi_selection_prob: 0.0
2024-05-14,13:56:48 | INFO |   rank: 0
2024-05-14,13:56:48 | INFO |   remote_sync: None
2024-05-14,13:56:48 | INFO |   remote_sync_frequency: 300
2024-05-14,13:56:48 | INFO |   remote_sync_protocol: s3
2024-05-14,13:56:48 | INFO |   report_to: wandb
2024-05-14,13:56:48 | INFO |   resume: None
2024-05-14,13:56:48 | INFO |   save_frequency: 1
2024-05-14,13:56:48 | INFO |   save_most_recent: False
2024-05-14,13:56:48 | INFO |   seed: 0
2024-05-14,13:56:48 | INFO |   siglip: False
2024-05-14,13:56:48 | INFO |   skip_scheduler: False
2024-05-14,13:56:48 | INFO |   tensorboard: False
2024-05-14,13:56:48 | INFO |   tensorboard_path: 
2024-05-14,13:56:48 | INFO |   torchcompile: False
2024-05-14,13:56:48 | INFO |   torchscript: False
2024-05-14,13:56:48 | INFO |   trace: False
2024-05-14,13:56:48 | INFO |   train_data: /projects/multimodal/datasets/pmc_oa/train.jsonl::/projects/multimodal/datasets/Quilt_1M/quilt_1m_train.csv::/projects/multimodal/datasets/mimic_cxr/mimic_cxr_double_image_train.csv::/projects/aieng/multimodal/datasets/roco/cache/radiologytraindata.csv
2024-05-14,13:56:48 | INFO |   train_data_upsampling_factors: None
2024-05-14,13:56:48 | INFO |   train_num_samples: 2769337
2024-05-14,13:56:48 | INFO |   use_bn_sync: False
2024-05-14,13:56:48 | INFO |   use_bnb_linear: None
2024-05-14,13:56:48 | INFO |   val_data: /projects/multimodal/datasets/pmc_oa/valid.jsonl
2024-05-14,13:56:48 | INFO |   val_frequency: 1
2024-05-14,13:56:48 | INFO |   val_no_retrieval: True
2024-05-14,13:56:48 | INFO |   val_num_samples: None
2024-05-14,13:56:48 | INFO |   wandb: True
2024-05-14,13:56:48 | INFO |   wandb_notes: 
2024-05-14,13:56:48 | INFO |   wandb_offline: False
2024-05-14,13:56:48 | INFO |   wandb_project_name: open-multi-modal
2024-05-14,13:56:48 | INFO |   warmup: 0
2024-05-14,13:56:48 | INFO |   wd: 0.1
2024-05-14,13:56:48 | INFO |   workers: 4
2024-05-14,13:56:48 | INFO |   world_size: 4
2024-05-14,13:56:48 | INFO |   zeroshot_frequency: 1
/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transform.py:378: UserWarning: Unused augmentation cfg items, specify `use_timm` to use (['scale', 'quilt_crop', 'num_views']).
  warnings.warn(f'Unused augmentation cfg items, specify `use_timm` to use ({list(aug_cfg_dict.keys())}).')
/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transform.py:378: UserWarning: Unused augmentation cfg items, specify `use_timm` to use (['scale', 'quilt_crop', 'num_views']).
  warnings.warn(f'Unused augmentation cfg items, specify `use_timm` to use ({list(aug_cfg_dict.keys())}).')
/fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/open_clip/transform.py:378: UserWarning: Unused augmentation cfg items, specify `use_timm` to use (['scale', 'quilt_crop', 'num_views']).
  warnings.warn(f'Unused augmentation cfg items, specify `use_timm` to use ({list(aug_cfg_dict.keys())}).')
2024-05-14,13:57:23 | INFO | Loaded mixed dataset with 2755782 samples.
2024-05-14,13:57:24 | INFO | Loaded mixed dataset with 2755782 samples.
2024-05-14,13:57:24 | INFO | Loaded mixed dataset with 2755782 samples.
2024-05-14,13:57:25 | INFO | Loaded mixed dataset with 2755782 samples.
2024-05-14,13:57:26 | INFO | Loaded mixed dataset with 164657 samples.
2024-05-14,13:57:26 | INFO | Loaded mixed dataset with 164657 samples.
2024-05-14,13:57:27 | INFO | Loaded mixed dataset with 164657 samples.
2024-05-14,13:57:27 | INFO | Loaded mixed dataset with 164657 samples.
wandb: Currently logged in as: yasaman-parhizkar (vector-institute-aieng). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /fs01/home/yaspar/Documents/GitHub/Multimodal/MedMultiModal/src/wandb/run-20240514_135729-HPtuning_batchsize_32
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HPtuning_batchsize_32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vector-institute-aieng/open-multi-modal
wandb: üöÄ View run at https://wandb.ai/vector-institute-aieng/open-multi-modal/runs/HPtuning_batchsize_32
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
2024-05-14,13:57:34 | INFO | Eval Epoch: 0 [32 / 164657]	Clip Loss: 3.443807	
2024-05-14,13:57:40 | INFO | Eval Epoch: 0 [3232 / 164657]	Clip Loss: 2.978262	
2024-05-14,13:57:46 | INFO | Eval Epoch: 0 [6432 / 164657]	Clip Loss: 2.880793	
2024-05-14,13:57:51 | INFO | Eval Epoch: 0 [9632 / 164657]	Clip Loss: 2.848809	
2024-05-14,13:57:57 | INFO | Eval Epoch: 0 [12832 / 164657]	Clip Loss: 2.844480	
2024-05-14,13:58:03 | INFO | Eval Epoch: 0 [16032 / 164657]	Clip Loss: 2.832227	
2024-05-14,13:58:09 | INFO | Eval Epoch: 0 [19232 / 164657]	Clip Loss: 2.820260	
2024-05-14,13:58:15 | INFO | Eval Epoch: 0 [22432 / 164657]	Clip Loss: 2.816212	
2024-05-14,13:58:21 | INFO | Eval Epoch: 0 [25632 / 164657]	Clip Loss: 2.813489	
2024-05-14,13:58:27 | INFO | Eval Epoch: 0 [28832 / 164657]	Clip Loss: 2.800209	
2024-05-14,13:58:32 | INFO | Eval Epoch: 0 [32032 / 164657]	Clip Loss: 2.789194	
2024-05-14,13:58:38 | INFO | Eval Epoch: 0 [35232 / 164657]	Clip Loss: 2.790086	
2024-05-14,13:58:44 | INFO | Eval Epoch: 0 [38432 / 164657]	Clip Loss: 2.786811	
2024-05-14,13:58:50 | INFO | Eval Epoch: 0 [41632 / 164657]	Clip Loss: 2.784453	
2024-05-14,13:58:55 | INFO | Eval Epoch: 0 [44832 / 164657]	Clip Loss: 2.778660	
2024-05-14,13:59:01 | INFO | Eval Epoch: 0 [48032 / 164657]	Clip Loss: 2.776090	
2024-05-14,13:59:07 | INFO | Eval Epoch: 0 [51232 / 164657]	Clip Loss: 2.771745	
2024-05-14,13:59:12 | INFO | Eval Epoch: 0 [54432 / 164657]	Clip Loss: 2.768853	
2024-05-14,13:59:18 | INFO | Eval Epoch: 0 [57632 / 164657]	Clip Loss: 2.770607	
2024-05-14,13:59:24 | INFO | Eval Epoch: 0 [60832 / 164657]	Clip Loss: 2.767988	
2024-05-14,13:59:29 | INFO | Eval Epoch: 0 [64032 / 164657]	Clip Loss: 2.763409	
2024-05-14,13:59:35 | INFO | Eval Epoch: 0 [67232 / 164657]	Clip Loss: 2.760106	
2024-05-14,13:59:40 | INFO | Eval Epoch: 0 [70432 / 164657]	Clip Loss: 2.757124	
2024-05-14,13:59:46 | INFO | Eval Epoch: 0 [73632 / 164657]	Clip Loss: 2.754993	
2024-05-14,13:59:52 | INFO | Eval Epoch: 0 [76832 / 164657]	Clip Loss: 2.751291	
2024-05-14,13:59:58 | INFO | Eval Epoch: 0 [80032 / 164657]	Clip Loss: 2.746823	
2024-05-14,14:00:04 | INFO | Eval Epoch: 0 [83232 / 164657]	Clip Loss: 2.743469	
2024-05-14,14:00:10 | INFO | Eval Epoch: 0 [86432 / 164657]	Clip Loss: 2.741024	
2024-05-14,14:00:16 | INFO | Eval Epoch: 0 [89632 / 164657]	Clip Loss: 2.738789	
2024-05-14,14:00:21 | INFO | Eval Epoch: 0 [92832 / 164657]	Clip Loss: 2.735525	
2024-05-14,14:00:27 | INFO | Eval Epoch: 0 [96032 / 164657]	Clip Loss: 2.731437	
2024-05-14,14:00:33 | INFO | Eval Epoch: 0 [99232 / 164657]	Clip Loss: 2.727720	
2024-05-14,14:00:39 | INFO | Eval Epoch: 0 [102432 / 164657]	Clip Loss: 2.725024	
2024-05-14,14:00:45 | INFO | Eval Epoch: 0 [105632 / 164657]	Clip Loss: 2.721568	
2024-05-14,14:00:50 | INFO | Eval Epoch: 0 [108832 / 164657]	Clip Loss: 2.720453	
2024-05-14,14:00:56 | INFO | Eval Epoch: 0 [112032 / 164657]	Clip Loss: 2.718557	
2024-05-14,14:01:03 | INFO | Eval Epoch: 0 [115232 / 164657]	Clip Loss: 2.714482	
2024-05-14,14:01:08 | INFO | Eval Epoch: 0 [118432 / 164657]	Clip Loss: 2.712878	
2024-05-14,14:01:14 | INFO | Eval Epoch: 0 [121632 / 164657]	Clip Loss: 2.710856	
2024-05-14,14:01:20 | INFO | Eval Epoch: 0 [124832 / 164657]	Clip Loss: 2.708394	
2024-05-14,14:01:26 | INFO | Eval Epoch: 0 [128032 / 164657]	Clip Loss: 2.706478	
2024-05-14,14:01:32 | INFO | Eval Epoch: 0 [131232 / 164657]	Clip Loss: 2.706368	
2024-05-14,14:01:38 | INFO | Eval Epoch: 0 [134432 / 164657]	Clip Loss: 2.706357	
2024-05-14,14:01:44 | INFO | Eval Epoch: 0 [137632 / 164657]	Clip Loss: 2.706536	
2024-05-14,14:01:50 | INFO | Eval Epoch: 0 [140832 / 164657]	Clip Loss: 2.704912	
2024-05-14,14:01:55 | INFO | Eval Epoch: 0 [144032 / 164657]	Clip Loss: 2.705282	
2024-05-14,14:02:01 | INFO | Eval Epoch: 0 [147232 / 164657]	Clip Loss: 2.705004	
2024-05-14,14:02:07 | INFO | Eval Epoch: 0 [150432 / 164657]	Clip Loss: 2.704330	
2024-05-14,14:02:13 | INFO | Eval Epoch: 0 [153632 / 164657]	Clip Loss: 2.704254	
2024-05-14,14:02:18 | INFO | Eval Epoch: 0 [156832 / 164657]	Clip Loss: 2.703786	
2024-05-14,14:02:24 | INFO | Eval Epoch: 0 [160032 / 164657]	Clip Loss: 2.704541	
2024-05-14,14:02:30 | INFO | Eval Epoch: 0 [163232 / 164657]	Clip Loss: 2.703780	
2024-05-14,14:02:33 | INFO | Eval Epoch: 0 clip_val_loss: 2.7026	epoch: 0.0000	num_samples: 164657.0000
2024-05-14,14:02:33 | INFO | Start epoch 0
2024-05-14,14:02:35 | INFO | Reducer buckets have been rebuilt in this iteration.
2024-05-14,14:02:35 | INFO | Reducer buckets have been rebuilt in this iteration.
2024-05-14,14:02:35 | INFO | Reducer buckets have been rebuilt in this iteration.
2024-05-14,14:02:35 | INFO | Reducer buckets have been rebuilt in this iteration.
2024-05-14,14:02:36 | INFO | Train Epoch: 0 [    512/2755782 (0%)] Data (t): 1.834 Batch (t): 3.230, 158.531/s, 39.6327/s/gpu LR: 0.000050 Logit Scale: 100.000 Contrastive_loss: 6.5870 (6.5870) Loss: 6.5870 (6.5870)
2024-05-14,14:04:57 | INFO | Train Epoch: 0 [  51712/2755782 (2%)] Data (t): 0.066 Batch (t): 1.415, 381.231/s, 95.3077/s/gpu LR: 0.000050 Logit Scale: 99.941 Contrastive_loss: 5.2071 (5.8970) Loss: 5.2071 (5.8970)
2024-05-14,14:07:17 | INFO | Train Epoch: 0 [ 102912/2755782 (4%)] Data (t): 0.066 Batch (t): 1.397, 366.788/s, 91.6971/s/gpu LR: 0.000050 Logit Scale: 99.923 Contrastive_loss: 4.9169 (5.5703) Loss: 4.9169 (5.5703)
2024-05-14,14:09:37 | INFO | Train Epoch: 0 [ 154112/2755782 (6%)] Data (t): 0.065 Batch (t): 1.398, 377.681/s, 94.4202/s/gpu LR: 0.000050 Logit Scale: 99.873 Contrastive_loss: 4.4257 (5.2842) Loss: 4.4257 (5.2842)
2024-05-14,14:11:57 | INFO | Train Epoch: 0 [ 205312/2755782 (7%)] Data (t): 0.066 Batch (t): 1.397, 371.191/s, 92.7977/s/gpu LR: 0.000050 Logit Scale: 99.821 Contrastive_loss: 4.2913 (5.0856) Loss: 4.2913 (5.0856)
2024-05-14,14:13:54 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_53013.jpg
2024-05-14,14:13:56 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_37937.jpg
2024-05-14,14:14:16 | INFO | Train Epoch: 0 [ 256512/2755782 (9%)] Data (t): 0.066 Batch (t): 1.393, 356.893/s, 89.2233/s/gpu LR: 0.000050 Logit Scale: 99.761 Contrastive_loss: 4.1244 (4.9254) Loss: 4.1244 (4.9254)
/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  "Palette images with Transparency expressed in bytes should be "
2024-05-14,14:15:21 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_26439.jpg
2024-05-14,14:16:02 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_07738.jpg
2024-05-14,14:16:35 | INFO | Train Epoch: 0 [ 307712/2755782 (11%)] Data (t): 0.066 Batch (t): 1.393, 373.099/s, 93.2747/s/gpu LR: 0.000050 Logit Scale: 99.706 Contrastive_loss: 3.8556 (4.7726) Loss: 3.8556 (4.7726)
2024-05-14,14:18:54 | INFO | Train Epoch: 0 [ 358912/2755782 (13%)] Data (t): 0.066 Batch (t): 1.391, 368.437/s, 92.1092/s/gpu LR: 0.000050 Logit Scale: 99.645 Contrastive_loss: 3.8971 (4.6631) Loss: 3.8971 (4.6631)
2024-05-14,14:21:14 | INFO | Train Epoch: 0 [ 410112/2755782 (15%)] Data (t): 0.065 Batch (t): 1.392, 367.389/s, 91.8472/s/gpu LR: 0.000050 Logit Scale: 99.579 Contrastive_loss: 3.7059 (4.5568) Loss: 3.7059 (4.5568)
2024-05-14,14:23:33 | INFO | Train Epoch: 0 [ 461312/2755782 (17%)] Data (t): 0.066 Batch (t): 1.395, 375.396/s, 93.8491/s/gpu LR: 0.000050 Logit Scale: 99.528 Contrastive_loss: 3.6880 (4.4699) Loss: 3.6880 (4.4699)
2024-05-14,14:24:00 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_50469.jpg
2024-05-14,14:24:42 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_07927.jpg
2024-05-14,14:25:52 | INFO | Train Epoch: 0 [ 512512/2755782 (19%)] Data (t): 0.066 Batch (t): 1.390, 359.078/s, 89.7694/s/gpu LR: 0.000050 Logit Scale: 99.456 Contrastive_loss: 3.5857 (4.3895) Loss: 3.5857 (4.3895)
2024-05-14,14:28:11 | INFO | Train Epoch: 0 [ 563712/2755782 (20%)] Data (t): 0.065 Batch (t): 1.391, 355.210/s, 88.8025/s/gpu LR: 0.000050 Logit Scale: 99.396 Contrastive_loss: 3.4196 (4.3087) Loss: 3.4196 (4.3087)
2024-05-14,14:30:30 | INFO | Train Epoch: 0 [ 614912/2755782 (22%)] Data (t): 0.065 Batch (t): 1.393, 361.500/s, 90.3751/s/gpu LR: 0.000050 Logit Scale: 99.341 Contrastive_loss: 3.4028 (4.2390) Loss: 3.4028 (4.2390)
2024-05-14,14:31:39 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_50644.jpg
2024-05-14,14:32:21 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_63050.jpg
2024-05-14,14:32:50 | INFO | Train Epoch: 0 [ 666112/2755782 (24%)] Data (t): 0.066 Batch (t): 1.399, 376.897/s, 94.2242/s/gpu LR: 0.000050 Logit Scale: 99.278 Contrastive_loss: 3.3621 (4.1764) Loss: 3.3621 (4.1764)
2024-05-14,14:33:45 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_67140.jpg
2024-05-14,14:35:10 | INFO | Train Epoch: 0 [ 717312/2755782 (26%)] Data (t): 0.065 Batch (t): 1.392, 359.046/s, 89.7614/s/gpu LR: 0.000050 Logit Scale: 99.222 Contrastive_loss: 3.3546 (4.1216) Loss: 3.3546 (4.1216)
2024-05-14,14:37:30 | INFO | Train Epoch: 0 [ 768512/2755782 (28%)] Data (t): 0.066 Batch (t): 1.400, 361.219/s, 90.3046/s/gpu LR: 0.000050 Logit Scale: 99.168 Contrastive_loss: 3.1471 (4.0607) Loss: 3.1471 (4.0607)
2024-05-14,14:39:49 | INFO | Train Epoch: 0 [ 819712/2755782 (30%)] Data (t): 0.066 Batch (t): 1.393, 361.955/s, 90.4888/s/gpu LR: 0.000050 Logit Scale: 99.124 Contrastive_loss: 3.3525 (4.0190) Loss: 3.3525 (4.0190)
2024-05-14,14:40:34 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_61045.jpg
2024-05-14,14:42:08 | INFO | Train Epoch: 0 [ 870912/2755782 (32%)] Data (t): 0.065 Batch (t): 1.394, 369.418/s, 92.3545/s/gpu LR: 0.000050 Logit Scale: 99.076 Contrastive_loss: 3.3723 (3.9831) Loss: 3.3723 (3.9831)
2024-05-14,14:44:27 | INFO | Train Epoch: 0 [ 922112/2755782 (33%)] Data (t): 0.065 Batch (t): 1.391, 388.371/s, 97.0927/s/gpu LR: 0.000050 Logit Scale: 99.028 Contrastive_loss: 3.2439 (3.9442) Loss: 3.2439 (3.9442)
2024-05-14,14:44:45 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_47259.jpg
2024-05-14,14:46:46 | INFO | Train Epoch: 0 [ 973312/2755782 (35%)] Data (t): 0.065 Batch (t): 1.389, 376.123/s, 94.0307/s/gpu LR: 0.000050 Logit Scale: 98.973 Contrastive_loss: 3.2265 (3.9083) Loss: 3.2265 (3.9083)
2024-05-14,14:49:05 | INFO | Train Epoch: 0 [1024512/2755782 (37%)] Data (t): 0.065 Batch (t): 1.388, 355.348/s, 88.8371/s/gpu LR: 0.000050 Logit Scale: 98.918 Contrastive_loss: 3.0029 (3.8652) Loss: 3.0029 (3.8652)
2024-05-14,14:50:05 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_64107.jpg
2024-05-14,14:51:24 | INFO | Train Epoch: 0 [1075712/2755782 (39%)] Data (t): 0.065 Batch (t): 1.394, 373.738/s, 93.4344/s/gpu LR: 0.000050 Logit Scale: 98.874 Contrastive_loss: 3.0461 (3.8280) Loss: 3.0461 (3.8280)
2024-05-14,14:53:15 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_78946.jpg
2024-05-14,14:53:44 | INFO | Train Epoch: 0 [1126912/2755782 (41%)] Data (t): 0.065 Batch (t): 1.394, 377.508/s, 94.3769/s/gpu LR: 0.000050 Logit Scale: 98.825 Contrastive_loss: 3.0244 (3.7930) Loss: 3.0244 (3.7930)
2024-05-14,14:55:30 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_24849.jpg
2024-05-14,14:56:03 | INFO | Train Epoch: 0 [1178112/2755782 (43%)] Data (t): 0.066 Batch (t): 1.388, 379.296/s, 94.8239/s/gpu LR: 0.000050 Logit Scale: 98.767 Contrastive_loss: 2.9937 (3.7597) Loss: 2.9937 (3.7597)
2024-05-14,14:58:22 | INFO | Train Epoch: 0 [1229312/2755782 (45%)] Data (t): 0.066 Batch (t): 1.390, 363.837/s, 90.9593/s/gpu LR: 0.000050 Logit Scale: 98.719 Contrastive_loss: 2.9314 (3.7266) Loss: 2.9314 (3.7266)
2024-05-14,15:00:41 | INFO | Train Epoch: 0 [1280512/2755782 (46%)] Data (t): 0.066 Batch (t): 1.390, 365.765/s, 91.4413/s/gpu LR: 0.000050 Logit Scale: 98.662 Contrastive_loss: 2.8681 (3.6936) Loss: 2.8681 (3.6936)
2024-05-14,15:03:00 | INFO | Train Epoch: 0 [1331712/2755782 (48%)] Data (t): 0.066 Batch (t): 1.395, 367.792/s, 91.9479/s/gpu LR: 0.000050 Logit Scale: 98.623 Contrastive_loss: 2.8894 (3.6638) Loss: 2.8894 (3.6638)
2024-05-14,15:05:19 | INFO | Train Epoch: 0 [1382912/2755782 (50%)] Data (t): 0.066 Batch (t): 1.392, 360.837/s, 90.2092/s/gpu LR: 0.000050 Logit Scale: 98.573 Contrastive_loss: 2.9664 (3.6389) Loss: 2.9664 (3.6389)
2024-05-14,15:06:25 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_63365.jpg
2024-05-14,15:07:39 | INFO | Train Epoch: 0 [1434112/2755782 (52%)] Data (t): 0.066 Batch (t): 1.395, 364.300/s, 91.0750/s/gpu LR: 0.000050 Logit Scale: 98.520 Contrastive_loss: 2.8530 (3.6118) Loss: 2.8530 (3.6118)
2024-05-14,15:09:59 | INFO | Train Epoch: 0 [1485312/2755782 (54%)] Data (t): 0.066 Batch (t): 1.398, 362.120/s, 90.5300/s/gpu LR: 0.000050 Logit Scale: 98.469 Contrastive_loss: 2.8616 (3.5868) Loss: 2.8616 (3.5868)
2024-05-14,15:12:18 | INFO | Train Epoch: 0 [1536512/2755782 (56%)] Data (t): 0.066 Batch (t): 1.392, 373.751/s, 93.4378/s/gpu LR: 0.000050 Logit Scale: 98.431 Contrastive_loss: 2.7146 (3.5586) Loss: 2.7146 (3.5586)
2024-05-14,15:13:41 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_77673.jpg
2024-05-14,15:13:45 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_35395.jpg
2024-05-14,15:14:35 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_60391.jpg
2024-05-14,15:14:37 | INFO | Train Epoch: 0 [1587712/2755782 (58%)] Data (t): 0.065 Batch (t): 1.395, 362.440/s, 90.6100/s/gpu LR: 0.000050 Logit Scale: 98.368 Contrastive_loss: 2.7101 (3.5321) Loss: 2.7101 (3.5321)
2024-05-14,15:15:08 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_25337.jpg
2024-05-14,15:16:56 | INFO | Train Epoch: 0 [1638912/2755782 (59%)] Data (t): 0.065 Batch (t): 1.388, 372.993/s, 93.2482/s/gpu LR: 0.000050 Logit Scale: 98.314 Contrastive_loss: 2.6653 (3.5059) Loss: 2.6653 (3.5059)
2024-05-14,15:19:15 | INFO | Train Epoch: 0 [1690112/2755782 (61%)] Data (t): 0.066 Batch (t): 1.393, 368.951/s, 92.2378/s/gpu LR: 0.000050 Logit Scale: 98.267 Contrastive_loss: 2.7476 (3.4836) Loss: 2.7476 (3.4836)
2024-05-14,15:21:34 | INFO | Train Epoch: 0 [1741312/2755782 (63%)] Data (t): 0.065 Batch (t): 1.388, 366.921/s, 91.7303/s/gpu LR: 0.000050 Logit Scale: 98.227 Contrastive_loss: 2.6900 (3.4609) Loss: 2.6900 (3.4609)
2024-05-14,15:23:53 | INFO | Train Epoch: 0 [1792512/2755782 (65%)] Data (t): 0.065 Batch (t): 1.391, 355.320/s, 88.8300/s/gpu LR: 0.000050 Logit Scale: 98.180 Contrastive_loss: 2.8181 (3.4430) Loss: 2.8181 (3.4430)
2024-05-14,15:26:12 | INFO | Train Epoch: 0 [1843712/2755782 (67%)] Data (t): 0.065 Batch (t): 1.387, 387.706/s, 96.9265/s/gpu LR: 0.000050 Logit Scale: 98.142 Contrastive_loss: 2.5706 (3.4194) Loss: 2.5706 (3.4194)
2024-05-14,15:28:31 | INFO | Train Epoch: 0 [1894912/2755782 (69%)] Data (t): 0.065 Batch (t): 1.390, 380.123/s, 95.0308/s/gpu LR: 0.000050 Logit Scale: 98.101 Contrastive_loss: 2.6360 (3.3988) Loss: 2.6360 (3.3988)
2024-05-14,15:30:50 | INFO | Train Epoch: 0 [1946112/2755782 (71%)] Data (t): 0.066 Batch (t): 1.393, 366.553/s, 91.6384/s/gpu LR: 0.000050 Logit Scale: 98.053 Contrastive_loss: 2.3535 (3.3720) Loss: 2.3535 (3.3720)
2024-05-14,15:32:34 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_26232.jpg
2024-05-14,15:32:43 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_57545.jpg
2024-05-14,15:33:10 | INFO | Train Epoch: 0 [1997312/2755782 (72%)] Data (t): 0.065 Batch (t): 1.395, 364.626/s, 91.1565/s/gpu LR: 0.000050 Logit Scale: 98.022 Contrastive_loss: 2.4659 (3.3494) Loss: 2.4659 (3.3494)
2024-05-14,15:35:28 | INFO | Train Epoch: 0 [2048512/2755782 (74%)] Data (t): 0.065 Batch (t): 1.383, 376.372/s, 94.0931/s/gpu LR: 0.000050 Logit Scale: 97.978 Contrastive_loss: 2.3956 (3.3261) Loss: 2.3956 (3.3261)
2024-05-14,15:35:34 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_40763.jpg
2024-05-14,15:37:47 | INFO | Train Epoch: 0 [2099712/2755782 (76%)] Data (t): 0.065 Batch (t): 1.389, 388.050/s, 97.0124/s/gpu LR: 0.000050 Logit Scale: 97.932 Contrastive_loss: 2.5007 (3.3065) Loss: 2.5007 (3.3065)
2024-05-14,15:40:06 | INFO | Train Epoch: 0 [2150912/2755782 (78%)] Data (t): 0.066 Batch (t): 1.389, 367.270/s, 91.8174/s/gpu LR: 0.000050 Logit Scale: 97.887 Contrastive_loss: 2.5112 (3.2880) Loss: 2.5112 (3.2880)
2024-05-14,15:40:22 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_70505.jpg
2024-05-14,15:42:25 | INFO | Train Epoch: 0 [2202112/2755782 (80%)] Data (t): 0.066 Batch (t): 1.389, 382.231/s, 95.5577/s/gpu LR: 0.000050 Logit Scale: 97.838 Contrastive_loss: 2.6168 (3.2727) Loss: 2.6168 (3.2727)
2024-05-14,15:44:44 | INFO | Train Epoch: 0 [2253312/2755782 (82%)] Data (t): 0.065 Batch (t): 1.395, 378.264/s, 94.5660/s/gpu LR: 0.000050 Logit Scale: 97.804 Contrastive_loss: 2.5779 (3.2573) Loss: 2.5779 (3.2573)
2024-05-14,15:47:04 | INFO | Train Epoch: 0 [2304512/2755782 (84%)] Data (t): 0.065 Batch (t): 1.397, 352.211/s, 88.0527/s/gpu LR: 0.000050 Logit Scale: 97.754 Contrastive_loss: 2.3363 (3.2373) Loss: 2.3363 (3.2373)
2024-05-14,15:47:11 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_36338.jpg
2024-05-14,15:49:24 | INFO | Train Epoch: 0 [2355712/2755782 (85%)] Data (t): 0.065 Batch (t): 1.396, 357.124/s, 89.2810/s/gpu LR: 0.000050 Logit Scale: 97.702 Contrastive_loss: 2.4785 (3.2211) Loss: 2.4785 (3.2211)
2024-05-14,15:50:28 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_24535.jpg
2024-05-14,15:51:43 | INFO | Train Epoch: 0 [2406912/2755782 (87%)] Data (t): 0.065 Batch (t): 1.393, 383.209/s, 95.8022/s/gpu LR: 0.000050 Logit Scale: 97.651 Contrastive_loss: 2.3798 (3.2036) Loss: 2.3798 (3.2036)
2024-05-14,15:52:47 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_22299.jpg
2024-05-14,15:54:02 | INFO | Train Epoch: 0 [2458112/2755782 (89%)] Data (t): 0.066 Batch (t): 1.387, 375.087/s, 93.7718/s/gpu LR: 0.000050 Logit Scale: 97.619 Contrastive_loss: 2.3631 (3.1864) Loss: 2.3631 (3.1864)
2024-05-14,15:56:21 | INFO | Train Epoch: 0 [2509312/2755782 (91%)] Data (t): 0.065 Batch (t): 1.395, 365.662/s, 91.4156/s/gpu LR: 0.000050 Logit Scale: 97.568 Contrastive_loss: 2.3906 (3.1705) Loss: 2.3906 (3.1705)
2024-05-14,15:58:40 | INFO | Train Epoch: 0 [2560512/2755782 (93%)] Data (t): 0.066 Batch (t): 1.388, 370.718/s, 92.6794/s/gpu LR: 0.000050 Logit Scale: 97.525 Contrastive_loss: 2.4676 (3.1567) Loss: 2.4676 (3.1567)
2024-05-14,16:00:59 | INFO | Train Epoch: 0 [2611712/2755782 (95%)] Data (t): 0.065 Batch (t): 1.395, 349.055/s, 87.2638/s/gpu LR: 0.000050 Logit Scale: 97.475 Contrastive_loss: 2.3620 (3.1414) Loss: 2.3620 (3.1414)
2024-05-14,16:02:28 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_29364.jpg
2024-05-14,16:02:43 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_07233.jpg
2024-05-14,16:03:18 | INFO | Train Epoch: 0 [2662912/2755782 (97%)] Data (t): 0.065 Batch (t): 1.386, 355.734/s, 88.9336/s/gpu LR: 0.000050 Logit Scale: 97.426 Contrastive_loss: 2.2710 (3.1250) Loss: 2.2710 (3.1250)
2024-05-14,16:05:37 | INFO | Train Epoch: 0 [2714112/2755782 (98%)] Data (t): 0.066 Batch (t): 1.392, 358.037/s, 89.5092/s/gpu LR: 0.000050 Logit Scale: 97.372 Contrastive_loss: 2.3486 (3.1106) Loss: 2.3486 (3.1106)
2024-05-14,16:07:30 | INFO | Train Epoch: 0 [2755584/2755782 (100%)] Data (t): 0.066 Batch (t): 1.390, 486.171/s, 121.543/s/gpu LR: 0.000050 Logit Scale: 97.348 Contrastive_loss: 2.2666 (3.0953) Loss: 2.2666 (3.0953)
2024-05-14,16:07:32 | INFO | Eval Epoch: 1 [32 / 164657]	Clip Loss: 1.446146	
2024-05-14,16:07:38 | INFO | Eval Epoch: 1 [3232 / 164657]	Clip Loss: 1.537663	
2024-05-14,16:07:44 | INFO | Eval Epoch: 1 [6432 / 164657]	Clip Loss: 1.471195	
2024-05-14,16:07:49 | INFO | Eval Epoch: 1 [9632 / 164657]	Clip Loss: 1.448471	
2024-05-14,16:07:55 | INFO | Eval Epoch: 1 [12832 / 164657]	Clip Loss: 1.445795	
2024-05-14,16:08:01 | INFO | Eval Epoch: 1 [16032 / 164657]	Clip Loss: 1.436489	
2024-05-14,16:08:06 | INFO | Eval Epoch: 1 [19232 / 164657]	Clip Loss: 1.430770	
2024-05-14,16:08:12 | INFO | Eval Epoch: 1 [22432 / 164657]	Clip Loss: 1.421347	
2024-05-14,16:08:18 | INFO | Eval Epoch: 1 [25632 / 164657]	Clip Loss: 1.413137	
2024-05-14,16:08:24 | INFO | Eval Epoch: 1 [28832 / 164657]	Clip Loss: 1.402159	
2024-05-14,16:08:29 | INFO | Eval Epoch: 1 [32032 / 164657]	Clip Loss: 1.395083	
2024-05-14,16:08:35 | INFO | Eval Epoch: 1 [35232 / 164657]	Clip Loss: 1.395236	
2024-05-14,16:08:41 | INFO | Eval Epoch: 1 [38432 / 164657]	Clip Loss: 1.393006	
2024-05-14,16:08:46 | INFO | Eval Epoch: 1 [41632 / 164657]	Clip Loss: 1.391465	
2024-05-14,16:08:52 | INFO | Eval Epoch: 1 [44832 / 164657]	Clip Loss: 1.389656	
2024-05-14,16:08:58 | INFO | Eval Epoch: 1 [48032 / 164657]	Clip Loss: 1.387824	
2024-05-14,16:09:03 | INFO | Eval Epoch: 1 [51232 / 164657]	Clip Loss: 1.386246	
2024-05-14,16:09:09 | INFO | Eval Epoch: 1 [54432 / 164657]	Clip Loss: 1.384139	
2024-05-14,16:09:14 | INFO | Eval Epoch: 1 [57632 / 164657]	Clip Loss: 1.385230	
2024-05-14,16:09:20 | INFO | Eval Epoch: 1 [60832 / 164657]	Clip Loss: 1.383416	
2024-05-14,16:09:25 | INFO | Eval Epoch: 1 [64032 / 164657]	Clip Loss: 1.380606	
2024-05-14,16:09:31 | INFO | Eval Epoch: 1 [67232 / 164657]	Clip Loss: 1.379418	
2024-05-14,16:09:36 | INFO | Eval Epoch: 1 [70432 / 164657]	Clip Loss: 1.379687	
2024-05-14,16:09:42 | INFO | Eval Epoch: 1 [73632 / 164657]	Clip Loss: 1.378133	
2024-05-14,16:09:48 | INFO | Eval Epoch: 1 [76832 / 164657]	Clip Loss: 1.375530	
2024-05-14,16:09:53 | INFO | Eval Epoch: 1 [80032 / 164657]	Clip Loss: 1.372719	
2024-05-14,16:09:59 | INFO | Eval Epoch: 1 [83232 / 164657]	Clip Loss: 1.373543	
2024-05-14,16:10:05 | INFO | Eval Epoch: 1 [86432 / 164657]	Clip Loss: 1.371197	
2024-05-14,16:10:10 | INFO | Eval Epoch: 1 [89632 / 164657]	Clip Loss: 1.369587	
2024-05-14,16:10:16 | INFO | Eval Epoch: 1 [92832 / 164657]	Clip Loss: 1.368674	
2024-05-14,16:10:22 | INFO | Eval Epoch: 1 [96032 / 164657]	Clip Loss: 1.367154	
2024-05-14,16:10:27 | INFO | Eval Epoch: 1 [99232 / 164657]	Clip Loss: 1.366323	
2024-05-14,16:10:33 | INFO | Eval Epoch: 1 [102432 / 164657]	Clip Loss: 1.364664	
2024-05-14,16:10:38 | INFO | Eval Epoch: 1 [105632 / 164657]	Clip Loss: 1.362432	
2024-05-14,16:10:44 | INFO | Eval Epoch: 1 [108832 / 164657]	Clip Loss: 1.361069	
2024-05-14,16:10:50 | INFO | Eval Epoch: 1 [112032 / 164657]	Clip Loss: 1.360441	
2024-05-14,16:10:56 | INFO | Eval Epoch: 1 [115232 / 164657]	Clip Loss: 1.359314	
2024-05-14,16:11:01 | INFO | Eval Epoch: 1 [118432 / 164657]	Clip Loss: 1.358784	
2024-05-14,16:11:07 | INFO | Eval Epoch: 1 [121632 / 164657]	Clip Loss: 1.357276	
2024-05-14,16:11:13 | INFO | Eval Epoch: 1 [124832 / 164657]	Clip Loss: 1.356052	
2024-05-14,16:11:18 | INFO | Eval Epoch: 1 [128032 / 164657]	Clip Loss: 1.354707	
2024-05-14,16:11:24 | INFO | Eval Epoch: 1 [131232 / 164657]	Clip Loss: 1.353216	
2024-05-14,16:11:30 | INFO | Eval Epoch: 1 [134432 / 164657]	Clip Loss: 1.352777	
2024-05-14,16:11:36 | INFO | Eval Epoch: 1 [137632 / 164657]	Clip Loss: 1.351634	
2024-05-14,16:11:41 | INFO | Eval Epoch: 1 [140832 / 164657]	Clip Loss: 1.352243	
2024-05-14,16:11:47 | INFO | Eval Epoch: 1 [144032 / 164657]	Clip Loss: 1.352831	
2024-05-14,16:11:53 | INFO | Eval Epoch: 1 [147232 / 164657]	Clip Loss: 1.353167	
2024-05-14,16:11:58 | INFO | Eval Epoch: 1 [150432 / 164657]	Clip Loss: 1.352589	
2024-05-14,16:12:04 | INFO | Eval Epoch: 1 [153632 / 164657]	Clip Loss: 1.352608	
2024-05-14,16:12:09 | INFO | Eval Epoch: 1 [156832 / 164657]	Clip Loss: 1.352697	
2024-05-14,16:12:15 | INFO | Eval Epoch: 1 [160032 / 164657]	Clip Loss: 1.353117	
2024-05-14,16:12:21 | INFO | Eval Epoch: 1 [163232 / 164657]	Clip Loss: 1.352634	
2024-05-14,16:12:23 | INFO | Eval Epoch: 1 clip_val_loss: 1.3517	epoch: 1.0000	num_samples: 164657.0000
2024-05-14,16:12:29 | INFO | Start epoch 1
2024-05-14,16:12:32 | INFO | Train Epoch: 1 [    512/2755782 (0%)] Data (t): 1.696 Batch (t): 2.821, 181.497/s, 45.3742/s/gpu LR: 0.000050 Logit Scale: 97.347 Contrastive_loss: 2.1846 (2.1846) Loss: 2.1846 (2.1846)
2024-05-14,16:13:29 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_29364.jpg
2024-05-14,16:14:06 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_40763.jpg
2024-05-14,16:14:45 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_57545.jpg
2024-05-14,16:14:51 | INFO | Train Epoch: 1 [  51712/2755782 (2%)] Data (t): 0.065 Batch (t): 1.398, 380.307/s, 95.0767/s/gpu LR: 0.000050 Logit Scale: 97.335 Contrastive_loss: 2.1688 (2.1767) Loss: 2.1688 (2.1767)
2024-05-14,16:17:10 | INFO | Train Epoch: 1 [ 102912/2755782 (4%)] Data (t): 0.065 Batch (t): 1.390, 353.789/s, 88.4473/s/gpu LR: 0.000050 Logit Scale: 97.308 Contrastive_loss: 2.1162 (2.1565) Loss: 2.1162 (2.1565)
2024-05-14,16:19:29 | INFO | Train Epoch: 1 [ 154112/2755782 (6%)] Data (t): 0.065 Batch (t): 1.387, 373.554/s, 93.3885/s/gpu LR: 0.000050 Logit Scale: 97.260 Contrastive_loss: 2.1067 (2.1441) Loss: 2.1067 (2.1441)
/fs01/home/yaspar/Documents/envs/mmm/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  "Palette images with Transparency expressed in bytes should be "
2024-05-14,16:21:49 | INFO | Train Epoch: 1 [ 205312/2755782 (7%)] Data (t): 0.065 Batch (t): 1.394, 356.521/s, 89.1303/s/gpu LR: 0.000050 Logit Scale: 97.213 Contrastive_loss: 2.0665 (2.1286) Loss: 2.0665 (2.1286)
2024-05-14,16:24:08 | INFO | Train Epoch: 1 [ 256512/2755782 (9%)] Data (t): 0.065 Batch (t): 1.391, 371.998/s, 92.9994/s/gpu LR: 0.000050 Logit Scale: 97.180 Contrastive_loss: 2.1328 (2.1293) Loss: 2.1328 (2.1293)
2024-05-14,16:26:26 | INFO | Train Epoch: 1 [ 307712/2755782 (11%)] Data (t): 0.065 Batch (t): 1.387, 355.492/s, 88.8729/s/gpu LR: 0.000050 Logit Scale: 97.138 Contrastive_loss: 2.0055 (2.1116) Loss: 2.0055 (2.1116)
2024-05-14,16:28:45 | INFO | Train Epoch: 1 [ 358912/2755782 (13%)] Data (t): 0.065 Batch (t): 1.391, 370.091/s, 92.5228/s/gpu LR: 0.000050 Logit Scale: 97.097 Contrastive_loss: 1.8990 (2.0850) Loss: 1.8990 (2.0850)
2024-05-14,16:29:39 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_50644.jpg
2024-05-14,16:31:05 | INFO | Train Epoch: 1 [ 410112/2755782 (15%)] Data (t): 0.065 Batch (t): 1.393, 359.840/s, 89.9599/s/gpu LR: 0.000050 Logit Scale: 97.060 Contrastive_loss: 2.0972 (2.0864) Loss: 2.0972 (2.0864)
2024-05-14,16:33:23 | INFO | Train Epoch: 1 [ 461312/2755782 (17%)] Data (t): 0.065 Batch (t): 1.384, 378.252/s, 94.5630/s/gpu LR: 0.000050 Logit Scale: 97.021 Contrastive_loss: 2.0641 (2.0842) Loss: 2.0641 (2.0842)
2024-05-14,16:34:57 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_07233.jpg
2024-05-14,16:35:42 | INFO | Train Epoch: 1 [ 512512/2755782 (19%)] Data (t): 0.065 Batch (t): 1.386, 373.132/s, 93.2831/s/gpu LR: 0.000050 Logit Scale: 96.968 Contrastive_loss: 2.1759 (2.0925) Loss: 2.1759 (2.0925)
2024-05-14,16:38:01 | INFO | Train Epoch: 1 [ 563712/2755782 (20%)] Data (t): 0.065 Batch (t): 1.390, 370.928/s, 92.7321/s/gpu LR: 0.000050 Logit Scale: 96.917 Contrastive_loss: 1.9674 (2.0821) Loss: 1.9674 (2.0821)
2024-05-14,16:39:31 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_26439.jpg
2024-05-14,16:40:19 | INFO | Train Epoch: 1 [ 614912/2755782 (22%)] Data (t): 0.065 Batch (t): 1.384, 375.967/s, 93.9918/s/gpu LR: 0.000050 Logit Scale: 96.871 Contrastive_loss: 1.9550 (2.0723) Loss: 1.9550 (2.0723)
2024-05-14,16:42:38 | INFO | Train Epoch: 1 [ 666112/2755782 (24%)] Data (t): 0.064 Batch (t): 1.391, 372.810/s, 93.2026/s/gpu LR: 0.000050 Logit Scale: 96.813 Contrastive_loss: 2.0521 (2.0708) Loss: 2.0521 (2.0708)
2024-05-14,16:44:58 | INFO | Train Epoch: 1 [ 717312/2755782 (26%)] Data (t): 0.065 Batch (t): 1.393, 379.273/s, 94.8184/s/gpu LR: 0.000050 Logit Scale: 96.787 Contrastive_loss: 2.1920 (2.0789) Loss: 2.1920 (2.0789)
2024-05-14,16:47:17 | INFO | Train Epoch: 1 [ 768512/2755782 (28%)] Data (t): 0.065 Batch (t): 1.391, 383.395/s, 95.8488/s/gpu LR: 0.000049 Logit Scale: 96.738 Contrastive_loss: 1.9297 (2.0696) Loss: 1.9297 (2.0696)
2024-05-14,16:49:36 | INFO | Train Epoch: 1 [ 819712/2755782 (30%)] Data (t): 0.065 Batch (t): 1.392, 365.628/s, 91.4071/s/gpu LR: 0.000049 Logit Scale: 96.681 Contrastive_loss: 1.9006 (2.0597) Loss: 1.9006 (2.0597)
2024-05-14,16:50:29 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_63365.jpg
2024-05-14,16:51:54 | INFO | Train Epoch: 1 [ 870912/2755782 (32%)] Data (t): 0.065 Batch (t): 1.386, 379.941/s, 94.9851/s/gpu LR: 0.000049 Logit Scale: 96.646 Contrastive_loss: 1.9196 (2.0519) Loss: 1.9196 (2.0519)
2024-05-14,16:54:13 | INFO | Train Epoch: 1 [ 922112/2755782 (33%)] Data (t): 0.065 Batch (t): 1.385, 365.003/s, 91.2507/s/gpu LR: 0.000049 Logit Scale: 96.606 Contrastive_loss: 1.9288 (2.0454) Loss: 1.9288 (2.0454)
2024-05-14,16:55:25 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_24849.jpg
2024-05-14,16:56:31 | INFO | Train Epoch: 1 [ 973312/2755782 (35%)] Data (t): 0.065 Batch (t): 1.385, 367.790/s, 91.9475/s/gpu LR: 0.000049 Logit Scale: 96.576 Contrastive_loss: 2.0079 (2.0435) Loss: 2.0079 (2.0435)
2024-05-14,16:58:04 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_61045.jpg
2024-05-14,16:58:51 | INFO | Train Epoch: 1 [1024512/2755782 (37%)] Data (t): 0.066 Batch (t): 1.395, 361.944/s, 90.4860/s/gpu LR: 0.000049 Logit Scale: 96.539 Contrastive_loss: 1.8600 (2.0348) Loss: 1.8600 (2.0348)
2024-05-14,17:01:10 | INFO | Train Epoch: 1 [1075712/2755782 (39%)] Data (t): 0.065 Batch (t): 1.389, 365.992/s, 91.4979/s/gpu LR: 0.000049 Logit Scale: 96.494 Contrastive_loss: 1.8903 (2.0282) Loss: 1.8903 (2.0282)
2024-05-14,17:03:30 | INFO | Train Epoch: 1 [1126912/2755782 (41%)] Data (t): 0.077 Batch (t): 1.396, 365.279/s, 91.3198/s/gpu LR: 0.000049 Logit Scale: 96.450 Contrastive_loss: 2.1384 (2.0330) Loss: 2.1384 (2.0330)
2024-05-14,17:04:43 | ERROR | Error loading image /projects/aieng/multimodal/datasets/roco/cache/images/ROCO_35395.jpg
2024-05-14,17:06:08 | INFO | Train Epoch: 1 [1178112/2755782 (43%)] Data (t): 0.089 Batch (t): 1.584, 306.066/s, 76.5165/s/gpu LR: 0.000049 Logit Scale: 96.417 Contrastive_loss: 1.9886 (2.0312) Loss: 1.9886 (2.0312)
2024-05-14,17:08:59 | INFO | Train Epoch: 1 [1229312/2755782 (45%)] Data (t): 0.106 Batch (t): 1.714, 290.786/s, 72.6964/s/gpu LR: 0.000049 Logit Scale: 96.375 Contrastive_loss: 1.9371 (2.0274) Loss: 1.9371 (2.0274)
srun: forcing job termination
srun: got SIGCONT
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
